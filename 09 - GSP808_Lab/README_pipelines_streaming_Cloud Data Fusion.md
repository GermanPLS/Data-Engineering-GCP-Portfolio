


# ‚ö° GSP808 - Building Realtime Pipelines in Cloud Data Fusion  
Este laboratorio ense√±a c√≥mo construir un pipeline de procesamiento de datos en tiempo real usando Cloud Data Fusion, integrando Pub/Sub como fuente de eventos y escribiendo los resultados en BigQuery.

##  Objetivo  
- Construir un pipeline de streaming con Cloud Data Fusion.  
- Leer mensajes JSON desde un t√≥pico de Pub/Sub.  
- Transformar los datos con Wrangler.  
- Escribir los resultados en BigQuery.

## üõ†Ô∏è Tecnolog√≠as utilizadas  
Cloud Data Fusion, Pub/Sub, BigQuery, Apache Spark Streaming, Wrangler

#
# Laboratorio

## Tarea 1. Permisos del proyecto

### Comprobar los permisos del proyecto

Antes de comenzar a trabajar en Google Cloud, debes asegurarte de que tu proyecto tenga los permisos correctos dentro de Administraci√≥n de identidad y acceso (IAM).

- haga clic en **IAM y administrador > IAM**.

- Confirme que la cuenta de servicio de computaci√≥n predeterminada `{project-number}-compute@developer.gserviceaccount.com` est√© presente y tenga el rol asignado de `editor`.

#
## Tarea 2. Aseg√∫rese de que la API de flujo de datos est√© habilitada correctamente


Para garantizar el acceso a la API necesaria, reinicie la conexi√≥n a la API de Dataflow.

1. En Cloud Console, escribe "Dataflow API" en la barra de b√∫squeda superior. Haz clic en el resultado " **Dataflow API**" .

2. Haga clic **Manage** .

3. Haga clic en **Disable API** .

    Si se le solicita que confirme, haga clic **Disable** .

4. Haga clic en **Enable** .

#
## Tarea 3. Cargar los datos


1. Primero, necesitas descargar los `tweets de muestra` a tu computadora. Luego los subir√°s con Wrangler para crear los pasos de transformaci√≥n.


    Tambi√©n necesitar√°s preparar el mismo archivo de tweets de muestra en tu contenedor de Cloud Storage. Hacia el final de este laboratorio, transmitir√°s los datos desde tu contenedor a un tema de Pub/Sub.

2. En Cloud Shell, ejecute los siguientes comandos para crear un nuevo dep√≥sito:


    ```bash
    export BUCKET=$GOOGLE_CLOUD_PROJECT
    gsutil mb gs://$BUCKET
    ```
    El dep√≥sito creado tiene el mismo nombre que el ID del proyecto.

3. Ejecute el siguiente comando para copiar el archivo de tweets en el dep√≥sito:

    ```bash
    gsutil cp gs://cloud-training/OCBL164/pubnub_tweets_2019-06-09-05-50_part-r-00000 gs://$BUCKET
    ```
4. Verifique que el archivo se haya copiado en su dep√≥sito de almacenamiento en la nube.

![alt text](1.png)
    

#
## Tarea 4. Configuraci√≥n del Pub/Sub Topic

Para utilizar un Pub/Sub, debe crear un topic para almacenar datos y una suscripci√≥n para acceder a los datos publicados en el tema.

1. En la consola en nube, desde el men√∫ de navegaci√≥n, haga clic en Ver todos los productos; en la secci√≥n An√°lisis, haga clic en Pub/Sub y luego seleccione Temas .

2. Haga clic en **Create topic.** .

3. El tema debe tener un nombre √∫nico. Para este laboratorio, as√≠gnele un nombre `cdf_lab_topicy` haga clic en **CREATE** 

    ![alt text](2.png)

.


#
## Tarea 5. Agregar una suscripci√≥n de Pub/Sub
Sigue en la p√°gina del tema. Ahora te suscribir√°s para acceder al tema.

1. Haga clic en **Create subscription.** .


2. Escriba un nombre para la suscripci√≥n, como `cdf_lab_subscription`, establezca el Tipo de entrega en **Pull** y luego haga clic en **Create** .


    ![alt text](3.png)

#
## Tarea 6. Agregue los permisos necesarios para su instancia de Cloud Data Fusion

- haz clic en "Ver todos los productos" y selecciona **Data Fusion > Instancias** . Deber√≠as ver una instancia de Cloud Data Fusion ya configurada y lista para usar.



- Abr√≠ el men√∫ de **IAM & Administraci√≥n** en la consola de GCP.

- Busc√° la cuenta de servicio predeterminada de Compute Engine : 

    `{project-number}-compute@developer.gserviceaccount.com`

   copie la **cuenta de servicio** en su portapapeles.          


- En la p√°gina Permisos de IAM, haga clic en **+ Otorgar acceso (+Grant Access)**.


- En el campo Nuevos principales (New principals field) pegue la cuenta de servicio.

- Haga clic en el campo **Select a role field** y comience a escribir **Cloud Data Fusion API Service Agent** ; luego selecci√≥nelo.

- Haga clic en **ADD ANOTHER ROLE** .

  A√±ade el rol `Dataproc Administrator `.

- Haga clic en **Guardar** .

### Otorgar permiso al usuario de la cuenta de servicio

- haga clic en **IAM y administraci√≥n > IAM **.

- Seleccione la casilla de verificaci√≥n  -- **Include Google-provided role grants**.

- Despl√°cese hacia abajo en la lista para encontrar la cuenta de servicio Cloud Data Fusion administrada por Google que se parece a `service-{project-number}@gcp-sa-datafusion.iam.gserviceaccount.comy` luego copie el nombre de la cuenta de servicio en su portapapeles.

- A continuaci√≥n, navegue hasta **IAM & admin > Service Accounts.** .

- Haga clic en la cuenta del motor de c√≥mputo predeterminada que se parece a `{project-number}-compute@developer.gserviceaccount.com` y seleccione la pesta√±a Principales con acceso en la navegaci√≥n superior.

- Haga clic en el bot√≥n **Grant Access** .

- En el campo **New Principals** , pegue la cuenta de servicio que copi√≥ anteriormente.

- En el men√∫ desplegable **Role** , seleccione **Service Account User.**

- Haga clic en **Guardar** 

#

## Tarea 7. Navegar por la interfaz de usuario de Cloud Data Fusion

Para navegar por la interfaz de usuario de Cloud Data Fusion, siga estos pasos:

1. En Cloud Console, vuelva a Data Fusion y haga clic en el enlace **"Ver instancia"** junto a su instancia de Data Fusion.

2. En el Centro de control de Cloud Data Fusion, use el men√∫ de Navegaci√≥n para exponer el men√∫ de la izquierda, luego elija **Pipeline > Studio** .

3. En la parte superior izquierda, utilice el men√∫ desplegable para seleccionar **Data Pipeline ‚Äì Realtime** .


    ![alt text](4.png)

#

## Tarea 8. Construir una canalizaci√≥n en tiempo real

Al trabajar con datos, es √∫til ver el aspecto de los datos sin procesar para usarlos como punto de partida para la transformaci√≥n. Para ello, utilizar√° Wrangler para preparar y limpiar los datos. Este enfoque centrado en los datos le permitir√° visualizar r√°pidamente sus transformaciones, y la retroalimentaci√≥n en tiempo real le permitir√° estar en el camino correcto.

1. En la secci√≥n Transformar de la paleta de plugins, seleccione **Wrangler** . El nodo Wrangler aparecer√° en el lienzo. **√Åbralo haciendo clic en el bot√≥n Propiedades**.

2. Haga clic en el **bot√≥n WRANGLE** debajo de la secci√≥n Directivas .

    ![alt text](4.2.png)

3. Cuando cargue, en el men√∫ lateral izquierdo, haz clic en "**Upload**" . A continuaci√≥n, haz clic en el icono de carga para subir el archivo de muestra de tweets que descargaste previamente a tu ordenador.


    ![alt text](6.png)

4. Los datos se cargan en la pantalla del Wrangler en formato de filas y columnas. Tardar√° un par de minutos.

5. La primera operaci√≥n consiste en analizar los datos JSON en una representaci√≥n tabular dividida en filas y columnas. Para ello, seleccione el icono desplegable del encabezado de la primera columna (cuerpo), luego seleccione la opci√≥n de men√∫ Analizar y, a continuaci√≥n, JSON en el submen√∫. En la ventana emergente, configure la profundidad en 1 y haga clic en Aplicar .

    ![alt text](9.png)

6. Repita el paso anterior para ver una estructura de datos m√°s significativa para la transformaci√≥n posterior. Haga clic en el icono desplegable de la columna **"body"** , seleccione **"Parse > JSON"** y configure la **Depth** en **1**. A continuaci√≥n, haga clic en **"Aplicar"** .

    ![alt text](10.png)


    Adem√°s de usar la interfaz de usuario, tambi√©n puedes escribir los pasos de transformaci√≥n en el cuadro de comandos de la directiva de Wrangler. Este cuadro aparece en la parte inferior de la interfaz de usuario de Wrangler (busca la consola de comandos con el s√≠mbolo del sistema $ en verde). Usar√°s la consola de comandos para pegar un conjunto de pasos de transformaci√≥n en el siguiente paso.

7. Agregue los pasos de transformaci√≥n a continuaci√≥n copi√°ndolos todos y peg√°ndolos en el cuadro de l√≠nea de comando de la directiva Wrangler:

    ```bash
    columns-replace s/^body_payload_//g
    drop id_str
    parse-as-simple-date :created_at EEE MMM dd HH:mm:ss Z yyyy
    drop display_text_range
    drop truncated
    drop in_reply_to_status_id_str
    drop in_reply_to_user_id_str
    parse-as-json :user 1
    drop coordinates
    set-type :place string
    drop geo,place,contributors,is_quote_status,favorited,retweeted,filter_level,user_id_str,user_url,user_description,user_translator_type,user_protected,user_verified,user_followers_count,user_friends_count,user_statuses_count,user_favourites_count,user_listed_count,user_is_translator,user_contributors_enabled,user_lang,user_geo_enabled,user_time_zone,user_utc_offset,user_created_at,user_profile_background_color,user_profile_background_image_url,user_profile_background_image_url_https,user_profile_background_tile,user_profile_link_color,user_profile_sidebar_border_color,user_profile_sidebar_fill_color,user_profile_text_color,user_profile_use_background_image
    drop user_following,user_default_profile_image,user_follow_request_sent,user_notifications,extended_tweet,quoted_status_id,quoted_status_id_str,quoted_status,quoted_status_permalink
    drop user_profile_image_url,user_profile_image_url_https,user_profile_banner_url,user_default_profile,extended_entities
    fill-null-or-empty :possibly_sensitive 'false'
    set-type :possibly_sensitive boolean
    drop :entities
    drop :user_location

    ```
   ![alt text](7.png)


        Nota: Si el mensaje "No hay datos. Intente eliminar algunos pasos de transformaci√≥n", elimine cualquiera de ellos haciendo clic en la X. Una vez que aparezcan los datos, podr√° continuar.


8. Haz clic en el bot√≥n **"Aplicar"** en la esquina superior derecha. Luego, haz clic en la **X** de la esquina superior derecha para cerrar el cuadro de propiedades.


    Como puede ver, ha vuelto a Pipeline Studio , donde se ha colocado un solo nodo en el lienzo que representa las transformaciones que acaba de definir en Wrangler .



9. En la secci√≥n **Source**de la paleta de plugins, seleccione **PubSub** . El nodo fuente de PubSub aparecer√° en el lienzo. √Åbralo haciendo clic en el bot√≥n **Propiedades** .

    Especifique las distintas propiedades de la fuente PubSub como se muestra a continuaci√≥n:

      - a. En **Nombre de referencia** , ingrese `Twitter_Input_Stream`

      - b. En **Suscripci√≥n**, ingrese `cdf_lab_subscription` (que es el nombre de su suscripci√≥n de PubSub que cre√≥ anteriormente)

    - c. Haga clic en **Validate**para asegurarse de que no se encontrar√°n errores.

    - d. Haga clic en la **X** en la parte superior derecha para cerrar el cuadro de propiedades.

    ![alt text](11.png)

10. Ahora conecte el nodo de origen PubSub al nodo Wrangler que agreg√≥ anteriormente.



11. Abra las propiedades de su nodo Wrangler y agregue la siguiente directiva en la parte superior de los Pasos de transformaci√≥n existentes:

    ```
    keep :message
    set-charset :message 'utf-8'
    rename :message :body
    ```


    Haga clic en la **X** en la parte superior derecha para cerrar el cuadro de propiedades.

    ![alt text](12.png)

12. Ahora que ha conectado una fuente y una transformaci√≥n a la canalizaci√≥n, compl√©tela a√±adiendo un receptor. En la secci√≥n "**Sink (Receptor)**" del panel izquierdo, seleccione **BigQuery** . Aparecer√° un nodo de receptor de BigQuery en el lienzo.

13. Conecte el nodo Wrangler al nodo BigQuery arrastrando la flecha desde el nodo Wrangler hasta el nodo BigQuery. A continuaci√≥n, configurar√° las propiedades del nodo BigQuery.

14. Coloque el cursor sobre el nodo BigQuery y haga clic en **Propiedades** .

    - a. En **Reference Name** , ingrese `realtime_pipeline`

    - b. En **Dataset** , ingrese `realtime`.

    - c. En la **Table** , ingrese `tweets`

    - d. Haga clic en **Validate** para asegurarse de que no se encontrar√°n errores.

15. Haga clic en la **X** en la parte superior derecha para cerrar el cuadro de propiedades.

    ![alt text](13.png)

16. Haga clic en Nombrar su canalizaci√≥n , agr√©guelo `Realtime_Pipeline` como nombre y haga clic en **Guardar** .

17. Haga clic en el √≠cono **Deploy** y luego inicie la canalizaci√≥n.

18. Una vez implementado, haga clic en  **Run**. Espere a que el **Status** de la canalizaci√≥n cambie a "Running" . Tardar√° un par de minutos.

#
## Tarea 9. Enviar mensajes a Cloud Pub/Sub

En esta tarea se simula la ingesta masiva de mensajes en un t√≥pico de Cloud Pub/Sub utilizando un trabajo de Dataflow basado en una plantilla predefinida.

### ¬øPara qu√© sirve?

En lugar de enviar mensaje por mensaje manualmente, se utiliza un archivo con datos (por ejemplo, tweets) almacenado en Cloud Storage, para publicarlos todos autom√°ticamente en un t√≥pico de Pub/Sub. Esto permite probar pipelines de streaming con datos reales y en volumen, emulando un flujo de eventos en tiempo real.

### Pasos principales:

1. En la consola de Google Cloud, ir a **Dataflow > Crear trabajo desde plantilla**.

2. Completar los campos:
   - **Nombre del trabajo:** `streaming-pipeline` (u otro nombre descriptivo).  
   - **Plantilla:** seleccionar **Text Files on Cloud Storage to Pub/Sub**.  
   - **Input Cloud Storage File(s):** ingresar la ruta del archivo en Cloud Storage, por ejemplo:  
     `gs://tu-bucket/pubnub_tweets_2019-06-09-05-50_part-r-00000`  
   - **Output Pub/Sub Topic:** indicar el t√≥pico de Pub/Sub previamente creado, con el formato:  
     `projects/tu-proyecto/topics/cdf_lab_topic`  
   - **Ubicaci√≥n temporal:** una carpeta temporal en Cloud Storage para uso interno por Dataflow, por ejemplo:  
     `gs://tu-bucket/tmp/`


    ![alt text](14.png)


3. Hacer clic en **Run job** para lanzar el proceso.

4. Dataflow leer√° l√≠nea a l√≠nea el archivo y publicar√° cada l√≠nea como un mensaje individual en Pub/Sub.

5. El pipeline de Cloud Data Fusion que est√© suscripto a ese t√≥pico empezar√° a recibir y procesar los mensajes en tiempo real.

### Beneficios:

- Simula un flujo continuo de datos para pruebas realistas.  
- Permite validar toda la arquitectura de ingesti√≥n y procesamiento streaming.  
- Facilita la automatizaci√≥n y escalabilidad en la generaci√≥n de eventos de prueba.

---

As√≠ pod√©s replicar cargas de datos reales para probar y ajustar tus pipelines streaming en Google Cloud.


#
## Tarea 10. Visualizaci√≥n de las m√©tricas de su pipeline
Tan pronto como los eventos se carguen en el tema Pub/Sub, deber√≠as comenzar a ver que son consumidos por la canalizaci√≥n: observa c√≥mo se actualizan las m√©tricas en cada nodo.

- En la consola de Data Fusion, espere a que cambien las m√©tricas de su canalizaci√≥n

![alt text](8.png)

#
# ¬°Felicidades!
En este laboratorio, aprendi√≥ a configurar una canalizaci√≥n en tiempo real en Data Fusion que lee mensajes entrantes de streaming desde Cloud Pub/Sub, procesa los datos y los escribe en BigQuery.

## üìé Recursos  
- Documentaci√≥n oficial: [https://cloud.google.com/data-fusion/docs](https://cloud.google.com/data-fusion/docs)  
- Enlace al lab oficial: [https://www.cloudskillsboost.google/focuses/12365](https://www.cloudskillsboost.google/focuses/12365?parent=catalog&utm_source=chatgpt.com)


#
